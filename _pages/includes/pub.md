<span class='anchor' id='-publications'></span>

# Publications

<div class='paper-box'>
* Equal Contribution&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;† Project Lead
</div>

## ♠ Innovator
<div class='paper-box'>
Coming Soon...
</div>

## ♠ Data Synthesis
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL (Findings) 2025</div><a href="https://arxiv.org/abs/2507.23440"><img src='images/paper/acl25_self-foveate.png' alt="Self-Foveate" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440), **Mingzhe Li**<sup>†</sup><span style="opacity: 0.6;">, Xin Lu, Yanyan Zhao</span>. [![](https://img.shields.io/github/stars/Mubuky/self-foveate?style=social&label=Code+Stars)](https://github.com/Mubuky/self-foveate)
- Proposes an automated **LLM-driven framework** named Self-Foveate for instruction synthesis from unsupervised text.
- Introduces a **“Micro-Scatter-Macro” multi-level foveation** methodology guiding LLMs to extract fine-grained and diverse information.
- Demonstrates **superior performance** across multiple unsupervised corpora and model architectures.
</div>
</div>

## ♠ Multimodal
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><a href="https://arxiv.org/abs/2511.04570"><img src='images/paper/arxiv_thinkingwithvideo.png' alt="Thinking with Video" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570), <span style="opacity: 0.6;">Jingqi Tong<sup>*</sup>, Yurong Mou<sup>*</sup>, Hangcheng Li<sup>*</sup>, </span>**Mingzhe Li**<sup>*</sup><span style="opacity: 0.6;">, Yongzhuo Yang<sup>*</sup>, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu</span>. <a href="https://huggingface.co/papers/2511.04570"><img src="https://img.shields.io/badge/%F0%9F%A4%97%201st%20paper%20of%20the%20week-yellow" alt="HuggingFace 1st paper of the week"></a> [![](https://img.shields.io/github/stars/tongjingqi/Thinking-with-Video?style=social&label=Code+Stars)](https://github.com/tongjingqi/Thinking-with-Video)
- Introduces **"Thinking with Video"**, a new paradigm unifying visual and textual reasoning through video generation models.
- Develops **VideoThinkBench**, a reasoning benchmark for video generation models covering both vision-centric and text-centric tasks.
- Demonstrates that Sora-2 **surpasses SOTA VLMs** on several tasks.
</div>
</div>

## ♠ LLM Safety
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2026</div><a href="https://arxiv.org/abs/2601.03537"><img src='images/paper/arxiv_star-s.png' alt="Thinking with Video" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules](https://arxiv.org/abs/2601.03537), <span style="opacity: 0.6;">Di Wu, Yanyan Zhao, Xin Lu, </span>**Mingzhe Li**<span style="opacity: 0.6;">, Bing Qin</span>. [![](https://img.shields.io/github/stars/pikepokenew/STAR_S?style=social&label=Code+Stars)](https://github.com/pikepokenew/STAR_S)
- Proposes **STAR-S**, a self-taught reasoning framework for safety alignment of LLMs.
- Introduces **Safety-Aware Reasoning** to guide LLMs to reason about safety rules during training.
- Demonstrates that STAR-S **improves safety alignment** on multiple benchmarks.
</div>
</div>
