<span class='anchor' id='-publications'></span>

# Publications

<div class='paper-box'>
* Equal Contribution&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Ä† Project Lead
</div>

## üéô Innovator

<div class='paper-box'>
Coming Soon...
</div>

## üéô Data Synthesis
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL (Findings) 2025</div><a href="https://arxiv.org/abs/2507.23440"><img src='images/paper/acl25_self-foveate.png' alt="Self-Foveate" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440), **Mingzhe Li**<sup>‚Ä†</sup><span style="opacity: 0.6;">, Xin Lu, Yanyan Zhao</span>. [![](https://img.shields.io/github/stars/Mubuky/self-foveate?style=social&label=Code+Stars)](https://github.com/Mubuky/self-foveate)
- Proposes an automated **LLM-driven framework** named Self-Foveate for instruction synthesis from unsupervised text.
- Introduces a **‚ÄúMicro-Scatter-Macro‚Äù multi-level foveation** methodology guiding LLMs to extract fine-grained and diverse information.
- Demonstrates **superior performance** across multiple unsupervised corpora and model architectures.
</div>
</div>

## üéô Multimodal
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><a href="https://arxiv.org/abs/2511.04570"><img src='images/paper/arxiv_thinkingwithvideo.png' alt="Thinking with Video" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">
[Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570), <span style="opacity: 0.6;">Jingqi Tong<sup>*</sup>, Yurong Mou<sup>*</sup>, Hangcheng Li<sup>*</sup>, </span>**Mingzhe Li**<sup>*</sup><span style="opacity: 0.6;">, Yongzhuo Yang<sup>*</sup>, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu</span>. <a href="https://huggingface.co/papers/2511.04570"><img src="https://img.shields.io/badge/%F0%9F%A4%97%201st%20paper%20of%20the%20day-yellow" alt="HuggingFace 1st paper of the day"></a> [![](https://img.shields.io/github/stars/tongjingqi/Thinking-with-Video?style=social&label=Code+Stars)](https://github.com/tongjingqi/Thinking-with-Video)
- Introduces **"Thinking with Video"**, a new paradigm unifying visual and textual reasoning through video generation models.
- Develops **VideoThinkBench**, a reasoning benchmark for video generation models covering both vision-centric and text-centric tasks.
- Demonstrates that Sora-2 **surpasses SOTA VLMs** on several tasks.
</div>
</div>
