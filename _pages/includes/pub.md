
<span class='anchor' id='-publications'></span>

# üìù Publications 
## üéô Data Synthesis
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Self-Foveate</div><img src='images/paper/acl25_self-foveate.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
`Findings of ACL 2025` [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440), **Mingzhe Li**, Xin Lu, Yanyan Zhao. [![](https://img.shields.io/github/stars/Mubuky/self-foveate?style=social&label=Code+Stars)](https://github.com/Mubuky/self-foveate)
- This work introduces an innovative **LLM-driven method** for instruction synthesis.
- Proposes a **"Micro-Scatter-Macro" multi-level foveation** methodology.
- Demonstrates **superior performance** across multiple unsupervised corpora and model architectures.
</div>
</div>


## üéô Multimodal
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Self-Foveate</div><img src='images/paper/acl25_self-foveate.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
`arXiv` [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2507.23440), Jingqi Tong<sup>*</sup>, Yurong Mou<sup>*</sup>, Hangcheng Li<sup>*</sup>, **Mingzhe Li**<sup>*</sup>, Yongzhuo Yang<sup>*</sup>, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu. [![](https://img.shields.io/github/stars/Mubuky/self-foveate?style=social&label=Code+Stars)](https://github.com/Mubuky/self-foveate)
- Introduces **‚ÄúThinking with Video‚Äù**, a new paradigm unifying visual and textual reasoning through video generation models.
- Develops **VideoThinkBench**, a benchmark covering both vision-centric and text-centric reasoning tasks.
- Demonstrates that Sora-2 **surpasses SOTA VLMs** on several tasks and shows strong reasoning enhanced by self-consistency and in-context learning.
</div>
</div>
