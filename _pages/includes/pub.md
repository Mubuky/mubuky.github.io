
<span class='anchor' id='-publications'></span>

# ğŸ“ Publications 
## ğŸ™ Data Synthesis
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Self-Foveate</div><img src='images/paper/acl25_self-foveate.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
`Findings of ACL 2025` [Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation](https://arxiv.org/abs/2507.23440), **Mingzhe Li**, Xin Lu, Yanyan Zhao. [![](https://img.shields.io/github/stars/Mubuky/self-foveate?style=social&label=Code+Stars)](https://github.com/Mubuky/self-foveate)
- Proposes an automated **LLM-driven framework** named Self-Foveate for instruction synthesis from unsupervised text.
- Introduces a **â€œMicro-Scatter-Macroâ€ multi-level foveation** methodology guiding LLMs to extract fine-grained and diverse information.
- Demonstrates **superior performance** across multiple unsupervised corpora and model architectures.
</div>
</div>


## ğŸ™ Multimodal
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Self-Foveate</div><img src='images\paper\arxiv_thinkingwithvideo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
`arXiv` [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2507.23440), Jingqi Tong<sup>*</sup>, Yurong Mou<sup>*</sup>, Hangcheng Li<sup>*</sup>, **Mingzhe Li**<sup>*</sup>, Yongzhuo Yang<sup>*</sup>, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu. [![](https://img.shields.io/github/stars/Mubuky/self-foveate?style=social&label=Code+Stars)](https://github.com/Mubuky/self-foveate)
- Introduces **â€œThinking with Videoâ€**, a new paradigm unifying visual and textual reasoning through video generation models.
- Develops **VideoThinkBench**, a reasoning benchmark for video generation models covering both vision-centric and text-centric tasks.
- Demonstrates that Sora-2 **surpasses SOTA VLMs** on several tasks.
</div>
</div>
